# In-Class Programming Activity, Day 15

Delia Walker-Jones
```{r}
library(glmnet)
load("mona.rda")

X <- t(mona) - mean(mona[])
X_rand <- matrix(rnorm(250*191,mean=0,sd=1), 250, 191)
X_corr <- X_rand %*% chol(var(X))

index <- sample(1:191)
zeros <- rep(0, 175)
c <- c(2, 5, -3, -4)
num <- sample(c, 16, replace = TRUE)
sparse_vector <- c(zeros, num)

beta <- sparse_vector[index]

Y_pure <- X %*% beta
Y_real <- (X %*% beta) + rnorm(250, mean = 0, sd = sqrt(var(Y_pure)))

modPure = lm(Y_pure ~ X)

v1 <- rep("X", 191) 
v2 <- as.character(c(1:191))
v3 <-paste0(v1, v2)

beta_hat_pure = coef(modPure)[v3]


plot(beta_hat_pure, beta)
```
We can see that beta_hat_pure is a perfectly linear model; when graphed against beta, it creates a perfect line.

```{r}
modReal = lm(Y_real ~ X)
beta_hat_real = coef(modReal)[v3]

plot(beta_hat_real, beta)
```
The range of beta_hat_real is much bigger than beta_hat_pure; we can see that beta is sparse because the vast majority of y-axis values fall at 0.

```{r}
lasso_mod <- cv.glmnet(X, Y_pure, alpha=1)
beta_lasso <- predict(lasso_mod, type = "coefficients", s = lasso_mod$lambda.min)

sing_vals <- svd(X)$d
r_square <- (cumsum(sing_vals^2))/sum(sing_vals^2)

sing_vals_rand <- svd(X_rand)$d
r_square_rand <- (cumsum(sing_vals_rand^2))/sum(sing_vals_rand^2)

sing_vals_corr <- svd(X_corr)$d
r_square_corr <- (cumsum(sing_vals_corr^2))/sum(sing_vals_corr^2)

plot(seq(1:191), r_square, col = "red")
points(seq(1:191), r_square_rand, col = "green")
points(seq(1:191), r_square_corr, col = "blue")

n99 <- 0
n99_rand <- 150
n99_corr <- 0

library(pls)
pcr.fit <- pcr(Y_real ~ X, scale = TRUE, validation="CV")
```



## Test statements

```{r}
scoreActivity::score253(15)
```