# Topic 4 Exercises: Classification

Delia Walker-Jones

```{r}
library("ISLR")
library("MASS")
library("class")
```


## 4.7.11

### A
```{r}
Auto01 <- data.frame(Auto)

Auto01$mpg01 <- ifelse(Auto$mpg>median(Auto$mpg) & Auto$mpg>median(Auto$mpg), 1, 0)
table(Auto01$mpg01)
```
### B
```{r}
names(Auto01)
plot(x = Auto01$acceleration, y = Auto01$mpg01)
plot(x = Auto01$weight, y = Auto01$mpg01)
plot(x = Auto01$year, y = Auto01$mpg01)
```
I looked briefly at acceleration, weight, and year to see how those might affect mpg. Acceleration appears to have some sort of relationship, with cars with lower acceleration having mpg below the median value. However, the acceleration variable appears pretty clustered in the 15-20mph range, so it might not be quite as interesting. I next looked at weight, which was interesting--lighter cars seem to be more likely to have mpg over the median, which seems counter-intuitive. There doesn't seem to be any relationship between year and mpg, so I went with weight and acceleration as my explanatory variables.

### C
```{r}
train = Auto01$year<82
test = Auto01[!train,]
mpg01Test <- Auto01$mpg01[!train]
```
In order to get more predictive data, I made the test dataset from all years except for 1982. Because year doesn't appear to affect mpg, this seems like a reasonable division. 

### D
```{r}
ldaMod = lda(mpg01 ~ weight + acceleration, data = Auto01, subset = train)
ldaMod
```
To test the model and see the test error, we run the model on our training data.
```{r}
ldaPred = predict(ldaMod, test)
ldaPred
table(ldaPred$class, mpg01Test)

# Test error rate
mean(ldaPred$class != mpg01Test)
```
We can conclude we have a test error rate of about 10%.

### E
```{r}
qdaMod = qda(mpg01 ~ weight + acceleration, data = Auto01, subset = train)
qdaMod

qdaPred = predict(qdaMod, test)
qdaPred

table(qdaPred$class, mpg01Test)

# Test error rate
mean(qdaPred$class != mpg01Test)
```
We have a test error of about 13.3%.

### F
```{r}
logMod = glm(mpg01 ~ weight + acceleration, data = Auto01, family = "binomial", subset = train)
summary(logMod)

logProb = predict(logMod, test, type = "response")
logPred = rep("Down", 30)
logPred[logProb >.5] = "Up"

table(logPred, mpg01Test)

# Test error rate
mean(logPred != mpg01Test)
```
### G
```{r}
trainX = cbind(Auto01$weight, Auto01$acceleration)[train,]
testX = cbind(Auto01$weight, Auto01$acceleration)[!train,]
trainMpg01 = Auto01$mpg01[train]


set.seed(1)
knnMod = knn(trainX, testX, trainMpg01, k=10)
summary(knnMod)
```

## 4.7.13
```{r}
Boston <- Boston
head(Boston)
summary(Boston$crim)
Boston$crim01 = ifelse(Boston$crim>median(Boston$crim) & Boston$crim>median(Boston$crim), 1, 0)
table(Boston$crim01)
```
We add a new variable, crim01, that consists of 1s and 0s representing crime rate above or below the median.

```{r}
set.seed(1)
train = sample(1:nrow(Boston), nrow(Boston)/2)
test = (-train)
crim01.test = Boston$crim01[test]
length(test)
```
We divide the data into test and training data.

```{r}
glmMod = glm(crim01 ~.-crim01 - crim, data = Boston[train,], family = "binomial")
summary(glmMod)

glmProb = predict(glmMod, Boston[test,], type = "response")
glmPred = rep(0, length(glmProb))
glmPred[glmProb > 0.5] <- 1
table(glmPred, crim01.test)

mean(glmPred != crim01.test)
```
Our error rate for logistic regression is about 12.25%.

```{r}
ldaMod = lda(crim01 ~.-crim01 - crim, data = Boston[train,])
ldaMod
ldaPred = predict(ldaMod, Boston[test,])
ldaClass = ldaPred$class
table(ldaClass, crim01.test)
mean(ldaClass != crim01.test)
```
For this LDA model, our error rate is about 16.6%.
```{r}
qdaMod = qda(crim01 ~.-crim01 - crim, data = Boston[train,])
qdaMod
qdaPred = predict(qdaMod, Boston[test,])
table(qdaPred$class, crim01.test)
mean(qdaPred$class != crim01.test)
```
For this QDA model, we have an error rate of about 11%.

```{r}
set.seed(1)
trainX = cbind(Boston$zn, Boston$indus, Boston$chas, Boston$nox, Boston$rm, Boston$age, Boston$dis, Boston$rad, Boston$tax, Boston$ptratio, Boston$black, Boston$lstat, Boston$medv)[train,]
testX = cbind(Boston$zn, Boston$indus, Boston$chas, Boston$nox, Boston$rm, Boston$age, Boston$dis, Boston$rad, Boston$tax, Boston$ptratio, Boston$black, Boston$lstat, Boston$medv)[test,]
knnPred = knn(trainX, testX, Boston$crim01[train], k = 1)
table(knnPred, crim01.test)

mean(knnPred != crim01.test)
```
The error rate for k = 1 is about 9%.

```{r}
knnPred1 = knn(trainX, testX, Boston$crim01[train], k = 10)
table(knnPred1, crim01.test)
mean(knnPred1 != crim01.test)
```
The error rate for k = 10 is about 15%.
```{r}
knnPred2 = knn(trainX, testX, Boston$crim01[train], k = 100)
table(knnPred2, crim01.test)
mean(knnPred2 != crim01.test)
```

The error rate for k = 100 is about 27.3%.


## 4.7.1
Prove that  
p(X) = e^(B~0~ + B~1~`*`X)/1 + e^(B~0~ + B~1~`*`X)  
is equivalent to  
p(X)/1 - p(X) = e^(B~0~ + B~1~`*`X)

p(X) `*` (1 + e^(B~0~ + B~1~`*`X)) = e^(B~0~ + B~1~`*`X)  
p(X) + (p(X) `*` e^(B~0~ + B~1~`*`X)) = e^(B~0~ + B~1~`*`X)
p(X) - e^(B~0~ + B~1~`*`X) = -(p(X) `*` e^(B~0~ + B~1~`*`X))  
e^(B~0~ + B~1~`*`X) = (p(X) `*` e^(B~0~ + B~1~`*`X)) - p(X)  
e^(B~0~ + B~1~`*`X) = p(X) `*` (e^(B~0~ + B~1~`*`X) - 1)  


## 4.7.8
The KNN (K = 1) approach, when used with in-sample testing--aka the training dataset--will result in 0% error, because the point is already present, so the "nearest neighbor" is predicted perfectly. However, because the error is 18% averaged over both the testing and training data sets, the error for the testing data is 36% (18 `*` 2). Therefore, the error for KNN is higher than that of the logistic regression model, and the logistic regression is the better fit for this particular data, suggesting that the decision boundaries are linear.

## 4.7.9

### A
p(X)/1 - p(X) = 0.37  
converted to  
p(X) = 0.37 / 1 + 0.37
```{r}
.37 / (1 + .37)
```
Therefore, p(X) = 0.27, meaning that 27% of people with an odds of .37 of defaulting will in fact default on their credit card payments.

### B
.16 / 1 - .16 = odds
```{r}
.16 / (1 - .16)
```

Therefore, the odds of a person defaulting with a 16% chance of defaulting on their credit card payment are .19.
